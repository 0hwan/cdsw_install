# Director 2.3 Config file
# Creates a small cluster including a Cloudera Data Science Workbench gateway node
#
#
# Copyright (c) 2016 Cloudera, Inc. All rights reserved.
#

#
# Simple AWS Cloudera Director configuration file with automatic role assignments
# that works as expected if you use a single instance type for all cluster nodes
#

#
# Cluster name
#
name: CDSW-with-kerberos

#
# Cloud provider configuration (credentials, region or zone and optional default image)
#

include file("./aws_provider.conf")

provider {
    type: aws

    accessKeyId: ${AWS_ACCESS_KEY_ID}
    secretAccessKey: ${AWS_SECRET_ACCESS_KEY}

    publishAccessKeys: false

    region: ${AWS_REGION} # an AWS region such as us-east-1

    subnetId: ${SUBNET_ID} # An AWS subnet, such as subnet-e7542291

    securityGroupsIds: ${SECURITY_GROUP_ID} # An AWS security group ID, such as sg-891a50f1 - ensure port 7189 is open!

    instanceNamePrefix: ${name}

    associatePublicIpAddresses: true
}

include file("ssh.conf")

common-instance-properties {
    image: ${AMI_ID} # An AWS AMI, 
    # (us-east - CDH 5.10; ami-5a25be4c - CDH 5.11 with JCE)
    # - best to build this as per https://github.com/cloudera/director-scripts/tree/master/faster-bootstrap
    # If this image includes CDH parcels then this image must be
    # consistent with the CDH parcel versions (e.g. don't use an AMI
    # that has the CDH 5.10 parcels if you select CDH 5.11 below!)
    tags {
        owner: ${OWNER}
    }
    bootstrapScripts: ["""
set -x
exec > ~/bootstrap.log 2>&1
if [ $(uname -r | grep -q el7) ]
then
	systemctl enable chronyd
	systemctl start chronyd
else
	service ntpd start
	chkconfig enable ntpd
fi

[ -z """${?KDC_HOST_IP}""" ] || {
# Not needed - using KDC_HOST_IP in the krb5.conf file directly
yum -y install krb5-workstation openldap-clients unzip
curl -O -j -k -L -H 'Cookie: oraclelicense=accept-securebackup-cookie' http://download.oracle.com/otn-pub/java/jce/7/UnlimitedJCEPolicyJDK7.zip
unzip -o -j -d /usr/java/jdk1.7.0_67-cloudera/jre/lib/security UnlimitedJCEPolicyJDK7.zip

}
"""]
}
#
# A list of instance types to use for group of nodes or management services
#

instances {
    master : ${common-instance-properties} {
        type: c4.xlarge
        instanceNamePrefix: master-${name}
    }
    worker : ${common-instance-properties} {
        type: c4.xlarge
#        type: c4.8xlarge
        instanceNamePrefix: worker-${name} 
    }
    cdsw : ${common-instance-properties} {
        instanceNamePrefix: cdsw-${name}
        type: c4.4xlarge
#        type: r4.16xlarge
	rootVolumeSizeGB: 100
	rootVolumeType: gp2
	ebsVolumeCount : 2
	ebsVolumeType: gp2
	ebsVolumeSizeGiB: 500
   }

}


#
# Configuration for Cloudera Manager. Cloudera Director can use an existing instance
# or bootstrap everything from scratch for a new cluster
#

include file("kerberos.conf")
cloudera-manager {

    instance: ${instances.master} {
    instanceNamePrefix: cm-${name}
        tags {
            application: "Cloudera Manager 5"
        }
    
   bootstrapScripts: ${instances.master.bootstrapScripts}
   }
    krbAdminUsername: ${?krbAdminUsername}
    krbAdminPassword: ${?krbAdminPassword}

   configs {
        # CLOUDERA_MANAGER corresponds to the Cloudera Manager Server configuration options

   	 CLOUDERA_MANAGER {
	    enable_api_debug: false
            custom_banner_html: "Managed by Cloudera Director"
            MANAGES_PARCELS: true
	    enable_faster_bootstrap: true
            KDC_TYPE: ${?KDC_TYPE}
	    AD_KDC_DOMAIN: ${?AD_KDC_DOMAIN}
	    KDC_HOST: ${?KDC_HOST_IP} # Using IP because of DSE-1796
            SECURITY_REALM: ${?SECURITY_REALM}
            KRB_MANAGE_KRB5_CONF: ${?KRB_MANAGE_KRB5_CONF}
	    # Note use of aes256 - need to ensure ALL cluster members have the necessary JCE policy files.
	    # If you find that you can get a ticket but not use it then this is likely the problem!
            KRB_ENC_TYPES: ${?KRB_ENC_TYPES}
   	}
    }

    csds: [
        "http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.1.0.cloudera1.jar"
    ]

    #
    # Automatically activate 60-Day Cloudera Enterprise Trial
    #
    enableEnterpriseTrial: true

    repository: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5.11/"
    repositoryKeyUrl: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/RPM-GPG-KEY-cloudera"

    postCreateScripts: ["""#!/bin/sh -x
    exec >~/postCreateScripts.log 2>&1
    yum -y install unzip
curl -O -j -k -L -H 'Cookie: oraclelicense=accept-securebackup-cookie' http://download.oracle.com/otn-pub/java/jce/7/UnlimitedJCEPolicyJDK7.zip
unzip -o -j -d /usr/java/jdk1.7.0_67-cloudera/jre/lib/security UnlimitedJCEPolicyJDK7.zip
"""]
}

#
# Cluster description
#

cluster {

    products {
      CDH: 5.11 # includes Hive and Spark
      SPARK2: 2
      Anaconda: 4
    }


    # S3 Configurations
    configs {
      HDFS {
        core_site_safety_valve: """
            <property>
                <name>fs.s3a.access.key</name>
                <value>"""${AWS_ACCESS_KEY_ID}"""</value>
            </property>
            <property>
                <name>fs.s3a.secret.key</name>
                <value>"""${AWS_SECRET_ACCESS_KEY}"""</value>
            </property>
            <property>
                <name>fs.s3a.block.size</name>
                <value>134217728</value>
            </property>
            <property>
              <name>fs.s3a.server-side-encryption-algorithm</name>
              <value>AES256</value>
            </property>
            <property>
              <name>fs.s3a.connection.ssl.enabled</name>
              <value>true</value>
              <description>Enables or disables SSL connections to S3.</description>
            </property>
        """
      }
    }

    parcelRepositories:
    ["http://archive.cloudera.com/cdh5/parcels/5.11/", 
    "https://archive.cloudera.com/spark2/parcels/2/",
    "https://repo.continuum.io/pkgs/misc/parcels/"
    ]

    services: [HDFS, YARN, SPARK2_ON_YARN, HIVE, OOZIE, IMPALA]

    masters {
      count: 1
      instance: ${instances.master}

      roles {
        HDFS: [NAMENODE, SECONDARYNAMENODE]
        YARN: [RESOURCEMANAGER, JOBHISTORY]
 	SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER]
        HIVE: [HIVESERVER2, HIVEMETASTORE]
        OOZIE: [OOZIE_SERVER]
	IMPALA: [CATALOGSERVER, STATESTORE]
      }
# Implement this manually until the root problem is fixed in Director
#      configs {YARN {RESOURCEMANAGER { "yarn.scheduler.maximum-allocation-mb": 2048 }	  }}
    }

    workers {
      count: 3

      instance: ${instances.worker}

      roles {
        HDFS: [DATANODE]
	IMPALA: [IMPALAD]
        YARN: [NODEMANAGER]
      }
#      configs {YARN { NODEMANAGER { "yarn.nodemanager.resource.memory-mb": 2048 } }}
    }
    gateways {
      count: 1

      instance: ${instances.cdsw} {
      	bootstrapScripts: ${instances.cdsw.bootstrapScripts} ["""
cd /etc/yum.repos.d
curl -O https://archive.cloudera.com/cdsw/1/redhat/7/x86_64/cdsw/cloudera-cdsw.repo
rpm --import https://archive.cloudera.com/cdsw/1/redhat/7/x86_64/cdsw/RPM-GPG-KEY-cloudera
yum -y install cloudera-data-science-workbench
exit 0
"""]
}

      roles {
        HDFS: [GATEWAY]
	YARN: [GATEWAY]
	SPARK2_ON_YARN: [GATEWAY]
      }
      configs {
            SPARK2_ON_YARN {
                GATEWAY {
                    "spark2-conf/spark-env.sh_client_config_safety_valve": """if [ -z ${PYSPARK_PYTHON} ]; then export PYSPARK_PYTHON=/opt/cloudera/parcels/Anaconda/bin/python spark-submit pyspark_script.py; fi"""
                }
            }
        }
    }
    instancePostCreateScripts:  ["""#!/bin/sh -x
exec >~/instancePostCreateScripts.log 2>&1
echo Starting instancePostCreateScript
echo Adding supergroup and cdsw user

groupadd supergroup
useradd -G supergroup -u 12354 hdfs_super
useradd -G supergroup -u 12345 cdsw
echo Cloudera1 | passwd --stdin cdsw

if rpm -q cloudera-data-science-workbench 
then
# Install java using alternatives:
alternatives --install /usr/bin/java java /usr/java/jdk1.7.0_67-cloudera/bin/java 2000

echo "This is the CDSW node"
# install git
yum -y install git

PUB_IP=$(curl http://169.254.169.254/latest/meta-data/public-ipv4)
DOM="ec2.${PUB_IP:?}.xip.io"
MASTER=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)
# Because we only added two disks to the instance then they'll be the
# first and last devices in /etc/fstab. Cut them out and use them for
# the Docker Block Devices (DBD) and teh Abpplication Block Device (ABD)
DBD="$(grep '^/dev' /etc/fstab | cut -f1 -d' ' | head -1)"
ABD="$(grep '^/dev' /etc/fstab | cut -f1 -d' ' | tail -1)"
sed -i -e "s/\(DOMAIN=\).*/\1${DOM:?}/" -e "s/\(MASTER_IP=\).*/\1${MASTER:?}/"  -e "s@\(DOCKER_BLOCK_DEVICES=\).*@\1\"${DBD:?}\"@" -e "s@\(APPLICATION_BLOCK_DEVICE=\).*@\1\"${ABD:?}\"@" /etc/cdsw/config/cdsw.conf
for dev in $(grep '^/dev' /etc/fstab | cut -f1 -d' '); do umount $dev; done
sed -i '/^\/dev/d' /etc/fstab

# Ensure that cdsw can restart after reboot
# Cannot use sysctl directly since the bridge module isn't loaded until after sysctl
# So load the module early and put the configuration so that sysctl can do its stuff
echo bridge > /etc/modules-load.d/bridge.conf
echo "net.bridge.bridge-nf-call-iptables=1" >>/etc/sysctl.d/bridge.conf
# CDSW prereq
# Ensure that the ipv6 networking is NOT disabled - this can be done at boot time:
echo "net.ipv6.conf.all.disable_ipv6=0" >>/etc/sysctl.conf
# Ensure that the rpcbind service is running
systemctl restart rpcbind
systemctl enable rpcbind

# init cdsw
# use 'echo' to get round the prompt indicating that the ulimit for
# files is not
# set to unlimited
echo | cdsw init

## Make sure that we add an auto restart script to the boot sequence
echo "cdsw restart" >> /etc/rc.d/rc.local
chmod a+x /etc/rc.d/rc.local

fi
## Not sure why but chronyd is always stopped at the end of this script!
systemctl start chronyd
exit 0
"""]
}
